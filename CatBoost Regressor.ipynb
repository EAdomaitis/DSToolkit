{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c90d1b",
   "metadata": {},
   "source": [
    "# CatBoost Regressor (Category Boosting)\n",
    "The goal of this script is to document the generic steps in hyper parameter tuning and training a CatBoost Regression model.\n",
    "This can be used to quickly produce a baseline model to compare to, but in practice, more modifications will be necessary for fine tuning and creating the best possible model.\n",
    "#### Useful Resources:\n",
    " - Source Documentation: https://catboost.ai/en/docs/\n",
    " - Informative Article on CatBoost: https://towardsdatascience.com/why-you-should-learn-catboost-now-390fb3895f76\n",
    " - Deep dive into important metrics and methods: https://coderzcolumn.com/tutorials/machine-learning/catboost-an-in-depth-guide-python#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ffbc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import catboost as cb\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53938a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_memory_usage optimizes the amount of memory required for each column. Pandas defaults to 64 bit data types, but this is wasteful.\n",
    "# This function looks at the range of values and assigns the most optimal type that keeps the data in tact\n",
    "\n",
    "# Source for this code: https://www.mikulskibartosz.name/how-to-reduce-memory-usage-in-pandas/\n",
    "def reduce_memory_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "    if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n",
    "                    df[col] = df[col].astype(np.uint8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n",
    "                    df[col] = df[col].astype(np.uint16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n",
    "                    df[col] = df[col].astype(np.uint32)                    \n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n",
    "                    df[col] = df[col].astype(np.uint64)\n",
    "            elif str(col_type)[:5] == 'float':\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49e9398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.42 MB\n",
      "Memory usage after optimization is: 1.30 MB\n",
      "Decreased by 8.3%\n"
     ]
    }
   ],
   "source": [
    "# Read in data and run reduce memory function\n",
    "# SIMPLE TEST EXAMPLE USING CALIFORNIA HOUSING DATASET\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "data = pd.DataFrame(housing.data)\n",
    "data.columns = housing.feature_names\n",
    "data['MedHouseVal'] = housing.target\n",
    "reduced_df = reduce_memory_usage(data)\n",
    "X, y = reduced_df.loc[:,reduced_df.columns != 'MedHouseVal'], reduced_df['MedHouseVal']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b32f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CatBoost_Regressor_Training (cbr_params, scoring_param, X_train, X_test, y_train, y_test):\n",
    "    # Parameter documentation: https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters\n",
    "    \n",
    "    # CatBoost specific data\n",
    "    train_dataset = cb.Pool(X_train, y_train) \n",
    "    test_dataset = cb.Pool(X_test, y_test)\n",
    "    \n",
    "    # Perform GridSearch\n",
    "    start_time = time.time()\n",
    "    cbr_tuned = cb.CatBoostRegressor(loss_function=scoring_param, verbose = False)\n",
    "    cbr_grid_result = cbr_tuned.grid_search(cbr_params, train_dataset, cv = 2)\n",
    "    \n",
    "    # Report Results\n",
    "    print('\\nGrid Search Completed in:', round(time.time() - start_time,0),'seconds')\n",
    "    print('Grid Search Best Parameters:',cbr_grid_result['params'],'\\n')\n",
    "    \n",
    "    # Create predictions on test dataset\n",
    "    cbr_preds = cbr_tuned.predict(X_test)\n",
    "    \n",
    "    # Store error metrics\n",
    "    cbr_error_metrics = {'mae':0,'rmse':0,'mse':0,'r2':0,'adjusted_r2':0}\n",
    "    cbr_error_metrics['mse'] = mean_squared_error(y_test, cbr_preds)\n",
    "    cbr_error_metrics['rmse'] = np.sqrt(cbr_error_metrics['mse'])\n",
    "    cbr_error_metrics['mae'] = mean_absolute_error(y_test, cbr_preds)\n",
    "    cbr_error_metrics['r2'] = r2_score(y_test,cbr_preds)\n",
    "    n = y_test.shape[0] # Number of rows\n",
    "    k = len(X_test.columns) # Number of independent variables\n",
    "    cbr_error_metrics['adjusted_r2'] = 1 - ((1-cbr_error_metrics['r2'])*(n-1)/(n-k-1)) # Adjusted R^2 calculation\n",
    "\n",
    "    # Print error metrics\n",
    "    print(\"\\n----------------- FINAL MODEL ERROR METRICS -----------------\")\n",
    "    print(\"MSE: %f\" % (cbr_error_metrics['mse']))\n",
    "    print(\"RMSE: %f\" % (cbr_error_metrics['rmse']))\n",
    "    print(\"MAE: %f\" % (cbr_error_metrics['mae']))\n",
    "    print(\"R Squared: %f\" % (cbr_error_metrics['r2']))\n",
    "    print(\"Adjusted R Squared: %f\" % (cbr_error_metrics['adjusted_r2']))\n",
    "    \n",
    "    # Returns the model, the best parameter list, predicted values, and common error metrics\n",
    "    return cbr_tuned, cbr_grid_result['params'], cbr_preds, cbr_error_metrics, cbr_tuned.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79411f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.7579154321\n",
      "bestIteration = 99\n",
      "\n",
      "0:\tloss: 0.7579154\tbest: 0.7579154 (0)\ttotal: 154ms\tremaining: 14.6s\n",
      "\n",
      "bestTest = 0.6129901106\n",
      "bestIteration = 99\n",
      "\n",
      "1:\tloss: 0.6129901\tbest: 0.6129901 (1)\ttotal: 289ms\tremaining: 13.6s\n",
      "\n",
      "bestTest = 0.7570627111\n",
      "bestIteration = 99\n",
      "\n",
      "2:\tloss: 0.7570627\tbest: 0.6129901 (1)\ttotal: 439ms\tremaining: 13.6s\n",
      "\n",
      "bestTest = 0.616525197\n",
      "bestIteration = 99\n",
      "\n",
      "3:\tloss: 0.6165252\tbest: 0.6129901 (1)\ttotal: 577ms\tremaining: 13.3s\n",
      "\n",
      "bestTest = 0.7574878695\n",
      "bestIteration = 99\n",
      "\n",
      "4:\tloss: 0.7574879\tbest: 0.6129901 (1)\ttotal: 713ms\tremaining: 13s\n",
      "\n",
      "bestTest = 0.6165594365\n",
      "bestIteration = 99\n",
      "\n",
      "5:\tloss: 0.6165594\tbest: 0.6129901 (1)\ttotal: 853ms\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.759740925\n",
      "bestIteration = 99\n",
      "\n",
      "6:\tloss: 0.7597409\tbest: 0.6129901 (1)\ttotal: 999ms\tremaining: 12.7s\n",
      "\n",
      "bestTest = 0.6169458889\n",
      "bestIteration = 99\n",
      "\n",
      "7:\tloss: 0.6169459\tbest: 0.6129901 (1)\ttotal: 1.16s\tremaining: 12.7s\n",
      "\n",
      "bestTest = 0.695351979\n",
      "bestIteration = 149\n",
      "\n",
      "8:\tloss: 0.6953520\tbest: 0.6129901 (1)\ttotal: 1.39s\tremaining: 13.4s\n",
      "\n",
      "bestTest = 0.5847935373\n",
      "bestIteration = 149\n",
      "\n",
      "9:\tloss: 0.5847935\tbest: 0.5847935 (9)\ttotal: 1.59s\tremaining: 13.7s\n",
      "\n",
      "bestTest = 0.6976287484\n",
      "bestIteration = 149\n",
      "\n",
      "10:\tloss: 0.6976287\tbest: 0.5847935 (9)\ttotal: 1.81s\tremaining: 14s\n",
      "\n",
      "bestTest = 0.5825840593\n",
      "bestIteration = 149\n",
      "\n",
      "11:\tloss: 0.5825841\tbest: 0.5825841 (11)\ttotal: 2.03s\tremaining: 14.2s\n",
      "\n",
      "bestTest = 0.6974976813\n",
      "bestIteration = 149\n",
      "\n",
      "12:\tloss: 0.6974977\tbest: 0.5825841 (11)\ttotal: 2.24s\tremaining: 14.3s\n",
      "\n",
      "bestTest = 0.5819468978\n",
      "bestIteration = 149\n",
      "\n",
      "13:\tloss: 0.5819469\tbest: 0.5819469 (13)\ttotal: 2.47s\tremaining: 14.5s\n",
      "\n",
      "bestTest = 0.6992094646\n",
      "bestIteration = 149\n",
      "\n",
      "14:\tloss: 0.6992095\tbest: 0.5819469 (13)\ttotal: 2.7s\tremaining: 14.6s\n",
      "\n",
      "bestTest = 0.5839845644\n",
      "bestIteration = 149\n",
      "\n",
      "15:\tloss: 0.5839846\tbest: 0.5819469 (13)\ttotal: 2.92s\tremaining: 14.6s\n",
      "\n",
      "bestTest = 0.6634447962\n",
      "bestIteration = 199\n",
      "\n",
      "16:\tloss: 0.6634448\tbest: 0.5819469 (13)\ttotal: 3.21s\tremaining: 14.9s\n",
      "\n",
      "bestTest = 0.5667771667\n",
      "bestIteration = 199\n",
      "\n",
      "17:\tloss: 0.5667772\tbest: 0.5667772 (17)\ttotal: 3.51s\tremaining: 15.2s\n",
      "\n",
      "bestTest = 0.6635655672\n",
      "bestIteration = 199\n",
      "\n",
      "18:\tloss: 0.6635656\tbest: 0.5667772 (17)\ttotal: 3.78s\tremaining: 15.3s\n",
      "\n",
      "bestTest = 0.5653013224\n",
      "bestIteration = 199\n",
      "\n",
      "19:\tloss: 0.5653013\tbest: 0.5653013 (19)\ttotal: 4.07s\tremaining: 15.4s\n",
      "\n",
      "bestTest = 0.6621957613\n",
      "bestIteration = 199\n",
      "\n",
      "20:\tloss: 0.6621958\tbest: 0.5653013 (19)\ttotal: 4.37s\tremaining: 15.6s\n",
      "\n",
      "bestTest = 0.5653962734\n",
      "bestIteration = 199\n",
      "\n",
      "21:\tloss: 0.5653963\tbest: 0.5653013 (19)\ttotal: 4.67s\tremaining: 15.7s\n",
      "\n",
      "bestTest = 0.6655998147\n",
      "bestIteration = 199\n",
      "\n",
      "22:\tloss: 0.6655998\tbest: 0.5653013 (19)\ttotal: 5.01s\tremaining: 15.9s\n",
      "\n",
      "bestTest = 0.5659751542\n",
      "bestIteration = 199\n",
      "\n",
      "23:\tloss: 0.5659752\tbest: 0.5653013 (19)\ttotal: 5.3s\tremaining: 15.9s\n",
      "\n",
      "bestTest = 0.66856935\n",
      "bestIteration = 99\n",
      "\n",
      "24:\tloss: 0.6685694\tbest: 0.5653013 (19)\ttotal: 5.56s\tremaining: 15.8s\n",
      "\n",
      "bestTest = 0.5524206187\n",
      "bestIteration = 99\n",
      "\n",
      "25:\tloss: 0.5524206\tbest: 0.5524206 (25)\ttotal: 5.79s\tremaining: 15.6s\n",
      "\n",
      "bestTest = 0.6687630466\n",
      "bestIteration = 99\n",
      "\n",
      "26:\tloss: 0.6687630\tbest: 0.5524206 (25)\ttotal: 6.03s\tremaining: 15.4s\n",
      "\n",
      "bestTest = 0.5497485978\n",
      "bestIteration = 99\n",
      "\n",
      "27:\tloss: 0.5497486\tbest: 0.5497486 (27)\ttotal: 6.26s\tremaining: 15.2s\n",
      "\n",
      "bestTest = 0.6688504192\n",
      "bestIteration = 99\n",
      "\n",
      "28:\tloss: 0.6688504\tbest: 0.5497486 (27)\ttotal: 6.48s\tremaining: 15s\n",
      "\n",
      "bestTest = 0.5511070183\n",
      "bestIteration = 99\n",
      "\n",
      "29:\tloss: 0.5511070\tbest: 0.5497486 (27)\ttotal: 6.7s\tremaining: 14.7s\n",
      "\n",
      "bestTest = 0.6679794096\n",
      "bestIteration = 99\n",
      "\n",
      "30:\tloss: 0.6679794\tbest: 0.5497486 (27)\ttotal: 6.91s\tremaining: 14.5s\n",
      "\n",
      "bestTest = 0.5511321429\n",
      "bestIteration = 99\n",
      "\n",
      "31:\tloss: 0.5511321\tbest: 0.5497486 (27)\ttotal: 7.12s\tremaining: 14.2s\n",
      "\n",
      "bestTest = 0.6112657112\n",
      "bestIteration = 149\n",
      "\n",
      "32:\tloss: 0.6112657\tbest: 0.5497486 (27)\ttotal: 7.43s\tremaining: 14.2s\n",
      "\n",
      "bestTest = 0.5242247073\n",
      "bestIteration = 149\n",
      "\n",
      "33:\tloss: 0.5242247\tbest: 0.5242247 (33)\ttotal: 7.73s\tremaining: 14.1s\n",
      "\n",
      "bestTest = 0.614320106\n",
      "bestIteration = 149\n",
      "\n",
      "34:\tloss: 0.6143201\tbest: 0.5242247 (33)\ttotal: 8.07s\tremaining: 14.1s\n",
      "\n",
      "bestTest = 0.5234936581\n",
      "bestIteration = 149\n",
      "\n",
      "35:\tloss: 0.5234937\tbest: 0.5234937 (35)\ttotal: 8.47s\tremaining: 14.1s\n",
      "\n",
      "bestTest = 0.6123801442\n",
      "bestIteration = 149\n",
      "\n",
      "36:\tloss: 0.6123801\tbest: 0.5234937 (35)\ttotal: 8.84s\tremaining: 14.1s\n",
      "\n",
      "bestTest = 0.5238794069\n",
      "bestIteration = 149\n",
      "\n",
      "37:\tloss: 0.5238794\tbest: 0.5234937 (35)\ttotal: 9.14s\tremaining: 14s\n",
      "\n",
      "bestTest = 0.6121730214\n",
      "bestIteration = 149\n",
      "\n",
      "38:\tloss: 0.6121730\tbest: 0.5234937 (35)\ttotal: 9.46s\tremaining: 13.8s\n",
      "\n",
      "bestTest = 0.5248186554\n",
      "bestIteration = 149\n",
      "\n",
      "39:\tloss: 0.5248187\tbest: 0.5234937 (35)\ttotal: 9.79s\tremaining: 13.7s\n",
      "\n",
      "bestTest = 0.5859984537\n",
      "bestIteration = 199\n",
      "\n",
      "40:\tloss: 0.5859985\tbest: 0.5234937 (35)\ttotal: 10.2s\tremaining: 13.7s\n",
      "\n",
      "bestTest = 0.5105104481\n",
      "bestIteration = 199\n",
      "\n",
      "41:\tloss: 0.5105104\tbest: 0.5105104 (41)\ttotal: 10.6s\tremaining: 13.6s\n",
      "\n",
      "bestTest = 0.5867688872\n",
      "bestIteration = 199\n",
      "\n",
      "42:\tloss: 0.5867689\tbest: 0.5105104 (41)\ttotal: 11s\tremaining: 13.5s\n",
      "\n",
      "bestTest = 0.5102662851\n",
      "bestIteration = 199\n",
      "\n",
      "43:\tloss: 0.5102663\tbest: 0.5102663 (43)\ttotal: 11.4s\tremaining: 13.5s\n",
      "\n",
      "bestTest = 0.5861038142\n",
      "bestIteration = 199\n",
      "\n",
      "44:\tloss: 0.5861038\tbest: 0.5102663 (43)\ttotal: 12s\tremaining: 13.6s\n",
      "\n",
      "bestTest = 0.5107597075\n",
      "bestIteration = 199\n",
      "\n",
      "45:\tloss: 0.5107597\tbest: 0.5102663 (43)\ttotal: 12.5s\tremaining: 13.6s\n",
      "\n",
      "bestTest = 0.5851964033\n",
      "bestIteration = 199\n",
      "\n",
      "46:\tloss: 0.5851964\tbest: 0.5102663 (43)\ttotal: 12.9s\tremaining: 13.5s\n",
      "\n",
      "bestTest = 0.5104281237\n",
      "bestIteration = 199\n",
      "\n",
      "47:\tloss: 0.5104281\tbest: 0.5102663 (43)\ttotal: 13.3s\tremaining: 13.3s\n",
      "\n",
      "bestTest = 0.6277900569\n",
      "bestIteration = 99\n",
      "\n",
      "48:\tloss: 0.6277901\tbest: 0.5102663 (43)\ttotal: 13.6s\tremaining: 13.1s\n",
      "\n",
      "bestTest = 0.5212664954\n",
      "bestIteration = 99\n",
      "\n",
      "49:\tloss: 0.5212665\tbest: 0.5102663 (43)\ttotal: 13.9s\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.627591257\n",
      "bestIteration = 99\n",
      "\n",
      "50:\tloss: 0.6275913\tbest: 0.5102663 (43)\ttotal: 14.2s\tremaining: 12.6s\n",
      "\n",
      "bestTest = 0.5231491595\n",
      "bestIteration = 99\n",
      "\n",
      "51:\tloss: 0.5231492\tbest: 0.5102663 (43)\ttotal: 14.5s\tremaining: 12.3s\n",
      "\n",
      "bestTest = 0.6309137521\n",
      "bestIteration = 99\n",
      "\n",
      "52:\tloss: 0.6309138\tbest: 0.5102663 (43)\ttotal: 14.9s\tremaining: 12.1s\n",
      "\n",
      "bestTest = 0.5260731838\n",
      "bestIteration = 99\n",
      "\n",
      "53:\tloss: 0.5260732\tbest: 0.5102663 (43)\ttotal: 15.2s\tremaining: 11.8s\n",
      "\n",
      "bestTest = 0.6314186381\n",
      "bestIteration = 99\n",
      "\n",
      "54:\tloss: 0.6314186\tbest: 0.5102663 (43)\ttotal: 15.5s\tremaining: 11.6s\n",
      "\n",
      "bestTest = 0.521914965\n",
      "bestIteration = 99\n",
      "\n",
      "55:\tloss: 0.5219150\tbest: 0.5102663 (43)\ttotal: 15.8s\tremaining: 11.3s\n",
      "\n",
      "bestTest = 0.5736372204\n",
      "bestIteration = 149\n",
      "\n",
      "56:\tloss: 0.5736372\tbest: 0.5102663 (43)\ttotal: 16.2s\tremaining: 11.1s\n",
      "\n",
      "bestTest = 0.5014681274\n",
      "bestIteration = 149\n",
      "\n",
      "57:\tloss: 0.5014681\tbest: 0.5014681 (57)\ttotal: 16.6s\tremaining: 10.9s\n",
      "\n",
      "bestTest = 0.5766047223\n",
      "bestIteration = 149\n",
      "\n",
      "58:\tloss: 0.5766047\tbest: 0.5014681 (57)\ttotal: 17.1s\tremaining: 10.7s\n",
      "\n",
      "bestTest = 0.5016478009\n",
      "bestIteration = 149\n",
      "\n",
      "59:\tloss: 0.5016478\tbest: 0.5014681 (57)\ttotal: 17.5s\tremaining: 10.5s\n",
      "\n",
      "bestTest = 0.5758108708\n",
      "bestIteration = 149\n",
      "\n",
      "60:\tloss: 0.5758109\tbest: 0.5014681 (57)\ttotal: 17.9s\tremaining: 10.3s\n",
      "\n",
      "bestTest = 0.5022003421\n",
      "bestIteration = 149\n",
      "\n",
      "61:\tloss: 0.5022003\tbest: 0.5014681 (57)\ttotal: 18.3s\tremaining: 10s\n",
      "\n",
      "bestTest = 0.5767634004\n",
      "bestIteration = 149\n",
      "\n",
      "62:\tloss: 0.5767634\tbest: 0.5014681 (57)\ttotal: 18.7s\tremaining: 9.8s\n",
      "\n",
      "bestTest = 0.4989388224\n",
      "bestIteration = 149\n",
      "\n",
      "63:\tloss: 0.4989388\tbest: 0.4989388 (63)\ttotal: 19.1s\tremaining: 9.56s\n",
      "\n",
      "bestTest = 0.5531336095\n",
      "bestIteration = 199\n",
      "\n",
      "64:\tloss: 0.5531336\tbest: 0.4989388 (63)\ttotal: 19.7s\tremaining: 9.38s\n",
      "\n",
      "bestTest = 0.488992318\n",
      "bestIteration = 199\n",
      "\n",
      "65:\tloss: 0.4889923\tbest: 0.4889923 (65)\ttotal: 20.2s\tremaining: 9.18s\n",
      "\n",
      "bestTest = 0.5550288082\n",
      "bestIteration = 199\n",
      "\n",
      "66:\tloss: 0.5550288\tbest: 0.4889923 (65)\ttotal: 20.7s\tremaining: 8.97s\n",
      "\n",
      "bestTest = 0.489530821\n",
      "bestIteration = 199\n",
      "\n",
      "67:\tloss: 0.4895308\tbest: 0.4889923 (65)\ttotal: 21.3s\tremaining: 8.76s\n",
      "\n",
      "bestTest = 0.554980017\n",
      "bestIteration = 199\n",
      "\n",
      "68:\tloss: 0.5549800\tbest: 0.4889923 (65)\ttotal: 21.9s\tremaining: 8.57s\n",
      "\n",
      "bestTest = 0.491212238\n",
      "bestIteration = 199\n",
      "\n",
      "69:\tloss: 0.4912122\tbest: 0.4889923 (65)\ttotal: 22.6s\tremaining: 8.38s\n",
      "\n",
      "bestTest = 0.5554439516\n",
      "bestIteration = 199\n",
      "\n",
      "70:\tloss: 0.5554440\tbest: 0.4889923 (65)\ttotal: 23.2s\tremaining: 8.16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.4880766751\n",
      "bestIteration = 199\n",
      "\n",
      "71:\tloss: 0.4880767\tbest: 0.4880767 (71)\ttotal: 23.9s\tremaining: 7.97s\n",
      "\n",
      "bestTest = 0.6036860859\n",
      "bestIteration = 99\n",
      "\n",
      "72:\tloss: 0.6036861\tbest: 0.4880767 (71)\ttotal: 24.6s\tremaining: 7.76s\n",
      "\n",
      "bestTest = 0.5122511032\n",
      "bestIteration = 99\n",
      "\n",
      "73:\tloss: 0.5122511\tbest: 0.4880767 (71)\ttotal: 25.3s\tremaining: 7.51s\n",
      "\n",
      "bestTest = 0.6016786048\n",
      "bestIteration = 99\n",
      "\n",
      "74:\tloss: 0.6016786\tbest: 0.4880767 (71)\ttotal: 25.8s\tremaining: 7.23s\n",
      "\n",
      "bestTest = 0.5131354756\n",
      "bestIteration = 99\n",
      "\n",
      "75:\tloss: 0.5131355\tbest: 0.4880767 (71)\ttotal: 26.3s\tremaining: 6.91s\n",
      "\n",
      "bestTest = 0.6023994411\n",
      "bestIteration = 99\n",
      "\n",
      "76:\tloss: 0.6023994\tbest: 0.4880767 (71)\ttotal: 26.8s\tremaining: 6.62s\n",
      "\n",
      "bestTest = 0.5116647047\n",
      "bestIteration = 99\n",
      "\n",
      "77:\tloss: 0.5116647\tbest: 0.4880767 (71)\ttotal: 27.3s\tremaining: 6.29s\n",
      "\n",
      "bestTest = 0.6072736828\n",
      "bestIteration = 99\n",
      "\n",
      "78:\tloss: 0.6072737\tbest: 0.4880767 (71)\ttotal: 27.8s\tremaining: 5.97s\n",
      "\n",
      "bestTest = 0.5117886894\n",
      "bestIteration = 99\n",
      "\n",
      "79:\tloss: 0.5117887\tbest: 0.4880767 (71)\ttotal: 28.2s\tremaining: 5.64s\n",
      "\n",
      "bestTest = 0.5560646094\n",
      "bestIteration = 149\n",
      "\n",
      "80:\tloss: 0.5560646\tbest: 0.4880767 (71)\ttotal: 29s\tremaining: 5.37s\n",
      "\n",
      "bestTest = 0.4905374914\n",
      "bestIteration = 148\n",
      "\n",
      "81:\tloss: 0.4905375\tbest: 0.4880767 (71)\ttotal: 30.1s\tremaining: 5.13s\n",
      "\n",
      "bestTest = 0.5550667664\n",
      "bestIteration = 149\n",
      "\n",
      "82:\tloss: 0.5550668\tbest: 0.4880767 (71)\ttotal: 30.8s\tremaining: 4.83s\n",
      "\n",
      "bestTest = 0.4925426461\n",
      "bestIteration = 149\n",
      "\n",
      "83:\tloss: 0.4925426\tbest: 0.4880767 (71)\ttotal: 31.6s\tremaining: 4.52s\n",
      "\n",
      "bestTest = 0.55573214\n",
      "bestIteration = 149\n",
      "\n",
      "84:\tloss: 0.5557321\tbest: 0.4880767 (71)\ttotal: 32.4s\tremaining: 4.19s\n",
      "\n",
      "bestTest = 0.4928887521\n",
      "bestIteration = 149\n",
      "\n",
      "85:\tloss: 0.4928888\tbest: 0.4880767 (71)\ttotal: 33.2s\tremaining: 3.86s\n",
      "\n",
      "bestTest = 0.5593854737\n",
      "bestIteration = 149\n",
      "\n",
      "86:\tloss: 0.5593855\tbest: 0.4880767 (71)\ttotal: 33.9s\tremaining: 3.51s\n",
      "\n",
      "bestTest = 0.4900479091\n",
      "bestIteration = 149\n",
      "\n",
      "87:\tloss: 0.4900479\tbest: 0.4880767 (71)\ttotal: 34.7s\tremaining: 3.15s\n",
      "\n",
      "bestTest = 0.536641987\n",
      "bestIteration = 199\n",
      "\n",
      "88:\tloss: 0.5366420\tbest: 0.4880767 (71)\ttotal: 35.6s\tremaining: 2.8s\n",
      "\n",
      "bestTest = 0.4821060723\n",
      "bestIteration = 199\n",
      "\n",
      "89:\tloss: 0.4821061\tbest: 0.4821061 (89)\ttotal: 36.6s\tremaining: 2.44s\n",
      "\n",
      "bestTest = 0.5353435345\n",
      "bestIteration = 199\n",
      "\n",
      "90:\tloss: 0.5353435\tbest: 0.4821061 (89)\ttotal: 37.5s\tremaining: 2.06s\n",
      "\n",
      "bestTest = 0.4852270524\n",
      "bestIteration = 199\n",
      "\n",
      "91:\tloss: 0.4852271\tbest: 0.4821061 (89)\ttotal: 38.7s\tremaining: 1.68s\n",
      "\n",
      "bestTest = 0.5363163861\n",
      "bestIteration = 199\n",
      "\n",
      "92:\tloss: 0.5363164\tbest: 0.4821061 (89)\ttotal: 39.8s\tremaining: 1.28s\n",
      "\n",
      "bestTest = 0.4845253477\n",
      "bestIteration = 198\n",
      "\n",
      "93:\tloss: 0.4845253\tbest: 0.4821061 (89)\ttotal: 40.9s\tremaining: 870ms\n",
      "\n",
      "bestTest = 0.5395581478\n",
      "bestIteration = 199\n",
      "\n",
      "94:\tloss: 0.5395581\tbest: 0.4821061 (89)\ttotal: 41.9s\tremaining: 441ms\n",
      "\n",
      "bestTest = 0.4807061371\n",
      "bestIteration = 199\n",
      "\n",
      "95:\tloss: 0.4807061\tbest: 0.4807061 (95)\ttotal: 42.8s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/2]\n",
      "\n",
      "bestTest = 0.4687193117\n",
      "bestIteration = 199\n",
      "\n",
      "Training on fold [1/2]\n",
      "\n",
      "bestTest = 0.4854349214\n",
      "bestIteration = 199\n",
      "\n",
      "\n",
      "Grid Search Completed in: 45.0 seconds\n",
      "Grid Search Best Parameters: {'depth': 8, 'l2_leaf_reg': 3, 'iterations': 200, 'learning_rate': 0.1} \n",
      "\n",
      "\n",
      "----------------- FINAL MODEL ERROR METRICS -----------------\n",
      "MSE: 0.217247\n",
      "RMSE: 0.466098\n",
      "MAE: 0.310950\n",
      "R Squared: 0.834393\n",
      "Adjusted R Squared: 0.834072\n"
     ]
    }
   ],
   "source": [
    "cbr_params = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "scoring_param = 'RMSE' # Options: https://catboost.ai/en/docs/concepts/loss-functions-regression#objectives-and-metrics\n",
    "\n",
    "# Call the function to run hyperparameter optimization and training of final model\n",
    "cbr_model, cbr_best_params, cbr_preds, cbr_error_metrics, cbr_feature_importances = CatBoost_Regressor_Training(cbr_params, scoring_param, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636e3c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score                :  {'learn': {'RMSE': 0.39319646862672164}}\n",
      "\n",
      "List of Target Classes  :  []\n",
      "\n",
      "Data Feature Names      :  ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "\n",
      "Feature Importance      :  [29.71196356  5.97091003  3.60099854  1.95843617  1.68856913 14.13978212\n",
      " 22.52625609 20.40308435]\n",
      "\n",
      "Learning Rate           :  0.10000000149011612\n",
      "\n",
      "Random Seed             :  0\n",
      "\n",
      "Number of Trees         :  200\n",
      "\n",
      "Number of Features      :  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score                : \",cbr_model.best_score_)\n",
    "print(\"\\nList of Target Classes  : \",cbr_model.classes_)\n",
    "print(\"\\nData Feature Names      : \",cbr_model.feature_names_)\n",
    "print(\"\\nFeature Importance      : \",cbr_model.feature_importances_)\n",
    "print(\"\\nLearning Rate           : \",cbr_model.learning_rate_)\n",
    "print(\"\\nRandom Seed             : \",cbr_model.random_seed_)\n",
    "print(\"\\nNumber of Trees         : \",cbr_model.tree_count_)\n",
    "print(\"\\nNumber of Features      : \",cbr_model.n_features_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acaaed8",
   "metadata": {},
   "source": [
    "#### Useful Methods\n",
    "**get_best_score()** - It returns best score of the estimator.\n",
    "\n",
    "**get_params()** - It returns parameters which were given as dictionary when creating CatBoost estimator and their values as dictionary.\n",
    "\n",
    "**get_all_params()** - It returns list of all parameters of CatBoost estimator and their values as dictionary.\n",
    "\n",
    "**get_cat_feature_indices()** - It returns list of indices which has categorical features.\n",
    "\n",
    "**get_feature_importance()** - It returns feature importance of individual feature according to trained model.\n",
    "\n",
    "**shrink(ntree_end, ntree_start=0)** - It accepts two arguments which are end tree and starts tree to shrink ensemble to include only trees that come in that index range discarding all other trees.\n",
    "\n",
    "**set_params()** - It can be used to set parameters of the estimator. Please make a note that this method will only work before the training model.\n",
    "\n",
    "**calc_leaf_indexes(data, ntree_start=0,ntree_end=0)** - It takes as input data and returns index of leaf in each tree which was used to make prediction for sample. The output of this function will be n_samples x n_trees. It'll return all trees' leaf index for a sample.\n",
    "\n",
    "**get_leaf_values()** - It returns actual leaf values of the trees in ensemble.\n",
    "\n",
    "**get_leaf_weights()** - It returns leaf weights for each leaf of the trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec61edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
